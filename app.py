import streamlit as st
from openai import OpenAI
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from pypdf import PdfReader # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è PDF

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="RAG PDF Chat", page_icon="üìÑ")
st.title("üìÑ –ß–∞—Ç —Å —Ç–≤–æ–∏–º PDF-—Ñ–∞–π–ª–æ–º")

#0 –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ "–ö—É—Ö–Ω–µ" (Ollama)
client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama',
)

# 1. –§—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è PDF
def get_pdf_text(uploaded_file):
    text = ""
    try:
        pdf_reader = PdfReader(uploaded_file)
        # –ß–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        for page in pdf_reader.pages:
            text += page.extract_text()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    return text

# 2. –§—É–Ω–∫—Ü–∏—è –Ω–∞—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ß–∞–Ω–∫–∏–Ω–≥)
def split_text(text, chunk_size=500, overlap=50):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        if len(chunk) > 50: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤—Å–µ–º –º–µ–ª–∫–∏–µ –∫—É—Å–æ—á–∫–∏
            chunks.append(chunk)
    return chunks
# 3. –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å)
def get_embedding(text):
    response = client.embeddings.create(
        model="all-minilm", 
        input=text
    )
    return response.data[0].embedding



# 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ü–∞–º—è—Ç–∏ (Session State)
# –°–∞–π—Ç –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∫–ª–∏–∫–µ. –ß—Ç–æ–±—ã —á–∞—Ç –Ω–µ –∏—Å—á–µ–∑–∞–ª,
# –º—ã —Ö—Ä–∞–Ω–∏–º –µ–≥–æ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–º —Ö—Ä–∞–Ω–∏–ª–∏—â–µ st.session_state.
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_db" not in st.session_state:
    st.session_state.vector_db = [] # –¢—É—Ç –±—É–¥–µ–º —Ö—Ä–∞–Ω–∏—Ç—å –≤–µ–∫—Ç–æ—Ä—ã —á–∞–Ω–∫–æ–≤



# –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
with st.sidebar:
    st.header("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª", type="pdf")
    
    if uploaded_file and not st.session_state.vector_db:
        with st.spinner("‚è≥ –ß–∏—Ç–∞—é –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ñ–∞–π–ª... (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)"):
            # –ê. –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç
            raw_text = get_pdf_text(uploaded_file)
            st.success(f"–ü—Ä–æ—á–∏—Ç–∞–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {len(raw_text)}")
            
            # –ë. –†–µ–∂–µ–º –Ω–∞ –∫—É—Å–æ—á–∫–∏
            chunks = split_text(raw_text)
            st.info(f"–ù–∞—Ä–µ–∑–∞–Ω–æ –Ω–∞ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.")
            
            # –í. –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–°–∞–º–æ–µ –¥–æ–ª–≥–æ–µ!)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å: {"text": –∫—É—Å–æ–∫_—Ç–µ–∫—Å—Ç–∞, "vector": –≤–µ–∫—Ç–æ—Ä}
            db = []
            progress_bar = st.progress(0)
            for i, chunk in enumerate(chunks):
                vector = get_embedding(chunk)
                db.append({"text": chunk, "vector": vector})
                progress_bar.progress((i + 1) / len(chunks))
            
            st.session_state.vector_db = db # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ø–∞–º—è—Ç—å —Å–µ—Å—Å–∏–∏
            st.success("‚úÖ –§–∞–π–ª –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω! –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã.")




# 4. –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
# –ü—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º—ã –ø—Ä–æ–±–µ–≥–∞–µ–º –ø–æ –ø–∞–º—è—Ç–∏ –∏ —Ä–∏—Å—É–µ–º –≤—Å–µ –ø—Ä–æ—à–ª—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 5. –ü–æ–ª–µ –≤–≤–æ–¥–∞ (–ñ–¥–µ–º, –ø–æ–∫–∞ —é–∑–µ—Ä –Ω–∞–ø–∏—à–µ—Ç –∏ –Ω–∞–∂–º–µ—Ç Enter)
if prompt := st.chat_input("–ù–∞–ø–∏—à–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."):
    
    # --- –î–ï–ô–°–¢–í–ò–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø ---
    # –ê. –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ —ç–∫—Ä–∞–Ω–µ
    with st.chat_message("user"):
        st.markdown(prompt)
    # –ë. –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–≥–æ –≤ –ø–∞–º—è—Ç—å
    st.session_state.messages.append({"role": "user", "content": prompt})


# 2. RAG: –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
    if st.session_state.vector_db:
        # –ê. –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –≤–æ–ø—Ä–æ—Å
        query_vector = get_embedding(prompt)
        
        # –ë. –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ —Å–æ –≤—Å–µ–º–∏ —á–∞–Ω–∫–∞–º–∏
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –∏–∑ –Ω–∞—à–µ–π –±–∞–∑—ã
        db_vectors = [item["vector"] for item in st.session_state.vector_db]
        similarities = cosine_similarity([query_vector], db_vectors)[0]
        
        # –í. –ë–µ—Ä–µ–º –¢–û–ü-3 –ª—É—á—à–∏—Ö –∫—É—Å–∫–∞
        top_indices = np.argsort(similarities)[-3:][::-1] # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –±–µ—Ä–µ–º 3 –ø–æ—Å–ª–µ–¥–Ω–∏—Ö (—Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö)
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫—É—Å–∫–æ–≤
        context_text = ""
        for idx in top_indices:
            score = similarities[idx]
            if score > 0.25: # –§–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–∞
                context_text += f"\n---\n{st.session_state.vector_db[idx]['text']}"
        
        # –ì. –§–æ—Ä–º–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        system_prompt = f"""
        –¢—ã –∞–Ω–∞–ª–∏—Ç–∏–∫. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å.
        –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–∫–∞–∂–∏ "–í –¥–æ–∫—É–º–µ–Ω—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–æ–º".
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞:
        {context_text}
        """
    else:
        # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –ø—Ä–æ—Å—Ç–æ –±–æ–ª—Ç–∞–µ–º
        system_prompt = "–¢—ã –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."

    # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        stream = client.chat.completions.create(
            model="mistral",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
        )

        
        # –ì. –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ –∫—É—Å–æ—á–∫–∞–º –∏ –æ–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ª–µ—Ç—É
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "‚ñå") # ‚ñå - —ç—Ç–æ –∫—É—Ä—Å–æ—Ä
        
        message_placeholder.markdown(full_response) # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫—É—Ä—Å–æ—Ä–∞
    
    # –î. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç –±–æ—Ç–∞ –≤ –ø–∞–º—è—Ç—å
    st.session_state.messages.append({"role": "assistant", "content": full_response})
