import streamlit as st
from openai import OpenAI
from pypdf import PdfReader # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è PDF
import chromadb 
import os
from chromadb.config import Settings
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer 

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="RAG Cloud Chat", page_icon="üìÑ")
st.title("‚òÅÔ∏è –ß–∞—Ç —Å PDF (OpenRouter + Local Embeddings)")

load_dotenv() 
api_key = os.getenv("OPENROUTER_API_KEY")

if not api_key:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω –∫–ª—é—á API! –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª .env –∏ –≤–ø–∏—à–∏—Ç–µ —Ç—É–¥–∞ OPENROUTER_API_KEY")
    st.stop()

# OpenRouter
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key,
)

@st.cache_resource# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –µ–¥–∏–Ω–æ—Ä–∞–∑–æ–≤–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ MiniLM
def load_embedding_model():
    return SentenceTransformer('all-MiniLM-L6-v2')

embedding_model = load_embedding_model()


chroma_client = chromadb.PersistentClient(path="my_vector_db")
collection = chroma_client.get_or_create_collection(
    name="my_documents",
    metadata={"hnsw:space": "cosine"} 
)

def get_pdf_text(uploaded_file):
    text = ""
    try:
        pdf_reader = PdfReader(uploaded_file)
        # –ß–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        for page in pdf_reader.pages:
            text += page.extract_text()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    return text


def split_text(text, chunk_size=500, overlap=100):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        if len(chunk) > 50: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤—Å–µ–º –º–µ–ª–∫–∏–µ –∫—É—Å–æ—á–∫–∏
            chunks.append(chunk)
    return chunks

def get_embedding(text):
    response = client.embeddings.create(
        model="all-minilm", 
        input=text
    )
    return response.data[0].embedding


def get_embedding(text):
    return embedding_model.encode(text).tolist()

if "messages" not in st.session_state:
    st.session_state.messages = []



with st.sidebar:
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª", type="pdf")
    
    if uploaded_file:
        filename = uploaded_file.name
        existing_docs = collection.get(where={"source": filename})
        
        if len(existing_docs['ids']) > 0:
            st.success(f"–§–∞–π–ª '{filename}' —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ.")
        else:
            with st.spinner("–ò–Ω–¥–µ–∫—Å–∏—Ä—É—é –Ω–æ–≤—ã–π —Ñ–∞–π–ª..."):
                text = get_pdf_text(uploaded_file)
                chunks = split_text(text)
                

                ids = []       
                metadatas = [] 
                vectors = []   
                documents_text = [] 
                
                progress = st.progress(0)
                for i, chunk in enumerate(chunks):
                    vec = get_embedding(chunk)
                    
                    ids.append(f"{filename}_chunk{i}")
                    metadatas.append({"source": filename})
                    vectors.append(vec)
                    documents_text.append(chunk)
                    
                    progress.progress((i+1)/len(chunks))
                

                collection.add(
                    ids=ids,
                    embeddings=vectors,
                    documents=documents_text, 
                    metadatas=metadatas
                )
                st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –±–∞–∑—É.")



for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("–í–æ–ø—Ä–æ—Å..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    query_vec = get_embedding(prompt)
    results = collection.query(
        query_embeddings=[query_vec],
        n_results=5
    )

    valid_chunks = []
    # –ù–∞–π–¥–µ–Ω–Ω–∞—è –≤ –±–∞–∑–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è 
    with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–ß—Ç–æ –Ω–∞—à–ª–∞ –±–∞–∑–∞)"):
        found_chunks = results['documents'][0]
        distances = results['distances'][0]
            
        for i, dist in enumerate(distances):
            chunk_text = found_chunks[i]
            st.write(f"**–ö—É—Å–æ–∫ {i+1}** (–î–∏—Å—Ç–∞–Ω—Ü–∏—è: {dist:.4f}):")
            st.caption(chunk_text[:200] + "...") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –∫—É—Å–∫–∞
                
            # –§–∏–ª—å—Ç—Ä: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –º–µ–Ω—å—à–µ 0.7 (–º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å)
            if dist < 0.7:
                st.success("–ü–æ–¥—Ö–æ–¥–∏—Ç")
                valid_chunks.append(chunk_text)
            else:
                st.warning("–≠—Ç–æ—Ç –∫—É—Å–æ–∫ –æ—Ç–±—Ä–æ—à–µ–Ω (—Å–ª–∏—à–∫–æ–º –Ω–µ–ø–æ—Ö–æ–∂)")

 
    if not valid_chunks:
        system_prompt = "–¢—ã —É–º–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    else:
        context_text = "\n---\n".join(valid_chunks)
        system_prompt = f"–û—Ç–≤–µ—Ç—å –∫–∞–∫ —É–º–Ω—ã–π –∏ –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_text}"

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (OpenRouter)
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        

        try:
            stream = client.chat.completions.create(
                model="meta-llama/llama-3.3-70b-instruct:free", # –ò–ª–∏ "google/gemma-2-9b-it:free"
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
                extra_headers={
                    "HTTP-Referer": "http://localhost:8501",
                    "X-Title": "Local RAG App"
                }
            )

            for chunk in stream:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "‚ñå") # ‚ñå - —ç—Ç–æ –∫—É—Ä—Å–æ—Ä
            message_placeholder.markdown(full_response) # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫—É—Ä—Å–æ—Ä–∞
            st.session_state.messages.append({"role": "assistant", "content": full_response})

        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ API: {e}")
        
