import streamlit as st
from openai import OpenAI
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity
from pypdf import PdfReader # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è PDF
import chromadb 
from chromadb.config import Settings


# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="RAG PDF Chat", page_icon="üìÑ")
st.title("üìÑ –ß–∞—Ç —Å —Ç–≤–æ–∏–º PDF-—Ñ–∞–π–ª–æ–º + –ø–∞–º—è—Ç—å")

#0 –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ "–ö—É—Ö–Ω–µ" (Ollama)
client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama',
)

# –ú—ã –≥–æ–≤–æ—Ä–∏–º: "–•—Ä–∞–Ω–∏ –¥–∞–Ω–Ω—ã–µ –≤ –ø–∞–ø–∫–µ 'my_vector_db' –ø—Ä—è–º–æ —Ç—É—Ç, –≤ –ø—Ä–æ–µ–∫—Ç–µ"
chroma_client = chromadb.PersistentClient(path="my_vector_db")
collection = chroma_client.get_or_create_collection(
    name="my_documents",
    metadata={"hnsw:space": "cosine"} 
)
# 1. –§—É–Ω–∫—Ü–∏—è —á—Ç–µ–Ω–∏—è PDF
def get_pdf_text(uploaded_file):
    text = ""
    try:
        pdf_reader = PdfReader(uploaded_file)
        # –ß–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        for page in pdf_reader.pages:
            text += page.extract_text()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    return text

# 2. –§—É–Ω–∫—Ü–∏—è –Ω–∞—Ä–µ–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ (–ß–∞–Ω–∫–∏–Ω–≥)
def split_text(text, chunk_size=500, overlap=50):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        if len(chunk) > 50: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤—Å–µ–º –º–µ–ª–∫–∏–µ –∫—É—Å–æ—á–∫–∏
            chunks.append(chunk)
    return chunks
# 3. –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞—Ä—å)
def get_embedding(text):
    response = client.embeddings.create(
        model="all-minilm", 
        input=text
    )
    return response.data[0].embedding


# –ë–û–ö–û–í–ê–Ø –ü–ê–ù–ï–õ–¨: –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
with st.sidebar:
    st.header("üìÇ –ó–∞–≥—Ä—É–∑–∫–∞")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª", type="pdf")
    
    if uploaded_file:
        filename = uploaded_file.name
        
        # –ü–†–û–í–ï–†–ö–ê: –ê –≤–¥—Ä—É–≥ –º—ã —ç—Ç–æ—Ç —Ñ–∞–π–ª —É–∂–µ —á–∏—Ç–∞–ª–∏?
        # –ú—ã –∏—â–µ–º –≤ –±–∞–∑–µ –∑–∞–ø–∏—Å–∏, –≥–¥–µ source == filename
        existing_docs = collection.get(where={"source": filename})
        
        if len(existing_docs['ids']) > 0:
            st.success(f"–§–∞–π–ª '{filename}' —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ.")
        else:
            with st.spinner("‚è≥ –ò–Ω–¥–µ–∫—Å–∏—Ä—É—é –Ω–æ–≤—ã–π —Ñ–∞–π–ª..."):
                text = get_pdf_text(uploaded_file)
                chunks = split_text(text)
                
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Chroma
                ids = []       # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –∫—É—Å–∫–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä "doc1_chunk0")
                metadatas = [] # –û–ø–∏—Å–∞–Ω–∏–µ (–æ—Ç–∫—É–¥–∞ –∫—É—Å–æ–∫)
                vectors = []   # –°–∞–º–∏ –≤–µ–∫—Ç–æ—Ä—ã
                documents_text = [] # –¢–µ–∫—Å—Ç –∫—É—Å–∫–æ–≤
                
                progress = st.progress(0)
                for i, chunk in enumerate(chunks):
                    vec = get_embedding(chunk)
                    
                    ids.append(f"{filename}_chunk{i}")
                    metadatas.append({"source": filename})
                    vectors.append(vec)
                    documents_text.append(chunk)
                    
                    progress.progress((i+1)/len(chunks))
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—ë –≤ –±–∞–∑—É –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
                collection.add(
                    ids=ids,
                    embeddings=vectors,
                    documents=documents_text, # Chroma —É–º–µ–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –∏ —Å–∞–º —Ç–µ–∫—Å—Ç!
                    metadatas=metadatas
                )
                st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–∞ –¥–∏—Å–∫!")


if "messages" not in st.session_state:
    st.session_state.messages = []
# 4. –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
# –ü—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –º—ã –ø—Ä–æ–±–µ–≥–∞–µ–º –ø–æ –ø–∞–º—è—Ç–∏ –∏ —Ä–∏—Å—É–µ–º –≤—Å–µ –ø—Ä–æ—à–ª—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("–í–æ–ø—Ä–æ—Å..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    query_vec = get_embedding(prompt)

    results = collection.query(
        query_embeddings=[query_vec],
        n_results=3
    )

    valid_chunks = []
    for i, dist in enumerate(results['distances'][0]):
        if dist < 0.9: # –ü–æ—Ä–æ–≥ (–Ω–∞–¥–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ)
            valid_chunks.append(results['documents'][0][i])

    if not valid_chunks:
    # –ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –∑–∞–ø—Ä–æ—Å –≤ Mistral –≤–æ–æ–±—â–µ!
        st.write("–í –±–∞–∑–µ –Ω–µ—Ç –Ω–∏—á–µ–≥–æ –ø–æ—Ö–æ–∂–µ–≥–æ.")

    
    context_text = "\n---\n".join(valid_chunks)
    if not valid_chunks:
        system_prompt = "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    else:
        system_prompt = f"–û—Ç–≤–µ—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_text}"

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        stream = client.chat.completions.create(
            model="mistral",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
        )

        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "‚ñå") # ‚ñå - —ç—Ç–æ –∫—É—Ä—Å–æ—Ä
        
        message_placeholder.markdown(full_response) # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫—É—Ä—Å–æ—Ä–∞
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
