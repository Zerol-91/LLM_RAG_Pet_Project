import streamlit as st
from openai import OpenAI
# import numpy as np
# from sklearn.metrics.pairwise import cosine_similarity
from pypdf import PdfReader # –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è —á—Ç–µ–Ω–∏—è PDF
import chromadb 
from chromadb.config import Settings


# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
st.set_page_config(page_title="RAG PDF Chat", page_icon="üìÑ")
st.title("üìÑ –ß–∞—Ç —Å —Ç–≤–æ–∏–º PDF-—Ñ–∞–π–ª–æ–º + –ø–∞–º—è—Ç—å")


client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama',
)


chroma_client = chromadb.PersistentClient(path="my_vector_db")
collection = chroma_client.get_or_create_collection(
    name="my_documents",
    metadata={"hnsw:space": "cosine"} 
)

def get_pdf_text(uploaded_file):
    text = ""
    try:
        pdf_reader = PdfReader(uploaded_file)
        # –ß–∏—Ç–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        for page in pdf_reader.pages:
            text += page.extract_text()
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è PDF: {e}")
    return text


def split_text(text, chunk_size=500, overlap=100):
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        if len(chunk) > 50: # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–æ–≤—Å–µ–º –º–µ–ª–∫–∏–µ –∫—É—Å–æ—á–∫–∏
            chunks.append(chunk)
    return chunks

def get_embedding(text):
    response = client.embeddings.create(
        model="all-minilm", 
        input=text
    )
    return response.data[0].embedding



with st.sidebar:
    st.header("–ó–∞–≥—Ä—É–∑–∫–∞")
    uploaded_file = st.file_uploader("–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª", type="pdf")
    
    if uploaded_file:
        filename = uploaded_file.name
        

        existing_docs = collection.get(where={"source": filename})
        
        if len(existing_docs['ids']) > 0:
            st.success(f"–§–∞–π–ª '{filename}' —É–∂–µ –µ—Å—Ç—å –≤ –±–∞–∑–µ.")
        else:
            with st.spinner("‚è≥ –ò–Ω–¥–µ–∫—Å–∏—Ä—É—é –Ω–æ–≤—ã–π —Ñ–∞–π–ª..."):
                text = get_pdf_text(uploaded_file)
                chunks = split_text(text)
                

                ids = []       
                metadatas = [] 
                vectors = []   
                documents_text = [] 
                
                progress = st.progress(0)
                for i, chunk in enumerate(chunks):
                    vec = get_embedding(chunk)
                    
                    ids.append(f"{filename}_chunk{i}")
                    metadatas.append({"source": filename})
                    vectors.append(vec)
                    documents_text.append(chunk)
                    
                    progress.progress((i+1)/len(chunks))
                

                collection.add(
                    ids=ids,
                    embeddings=vectors,
                    documents=documents_text, 
                    metadatas=metadatas
                )
                st.success("–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–∞ –¥–∏—Å–∫!")


if "messages" not in st.session_state:
    st.session_state.messages = []


for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("–í–æ–ø—Ä–æ—Å..."):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    query_vec = get_embedding(prompt)

    results = collection.query(
        query_embeddings=[query_vec],
        n_results=5
    )

    valid_chunks = []
    for i, dist in enumerate(results['distances'][0]):
        if dist < 0.7: # –ü–æ—Ä–æ–≥ (–Ω–∞–¥–æ –ø–æ–¥–±–∏—Ä–∞—Ç—å —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ)
            valid_chunks.append(results['documents'][0][i])

    if not valid_chunks:
        st.write("–í –±–∞–∑–µ –Ω–µ—Ç –Ω–∏—á–µ–≥–æ –ø–æ—Ö–æ–∂–µ–≥–æ.")

    
    with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–ß—Ç–æ –Ω–∞—à–ª–∞ –±–∞–∑–∞)"):
        found_chunks = results['documents'][0]
        distances = results['distances'][0]
            
        context_text = ""
        for i, chunk in enumerate(found_chunks):
            dist = distances[i]
            st.write(f"**–ö—É—Å–æ–∫ {i+1}** (–î–∏—Å—Ç–∞–Ω—Ü–∏—è: {dist:.4f}):")
            st.caption(chunk[:200] + "...") # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞—á–∞–ª–æ –∫—É—Å–∫–∞
                
            # –§–∏–ª—å—Ç—Ä: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–∏—Å—Ç–∞–Ω—Ü–∏—è –º–µ–Ω—å—à–µ 0.7 (–º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å)
            if dist < 0.7:
                context_text += f"\n---\n{chunk}"
            else:
                st.warning("–≠—Ç–æ—Ç –∫—É—Å–æ–∫ –æ—Ç–±—Ä–æ—à–µ–Ω (—Å–ª–∏—à–∫–æ–º –Ω–µ–ø–æ—Ö–æ–∂)")



    context_text = "\n---\n".join(valid_chunks)
    if not valid_chunks:
        system_prompt = "–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç."
    else:
        system_prompt = f"–û—Ç–≤–µ—Ç—å, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_text}"

    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        stream = client.chat.completions.create(
            model="mistral",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            stream=True,
        )

        
        for chunk in stream:
            if chunk.choices[0].delta.content:
                full_response += chunk.choices[0].delta.content
                message_placeholder.markdown(full_response + "‚ñå") # ‚ñå - —ç—Ç–æ –∫—É—Ä—Å–æ—Ä
        
        message_placeholder.markdown(full_response) # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –±–µ–∑ –∫—É—Ä—Å–æ—Ä–∞
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
