# import numpy as np
# from openai import OpenAI
# from sklearn.metrics.pairwise import cosine_similarity

# # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–∞
# client = OpenAI(
#     base_url='http://localhost:11434/v1',
#     api_key='ollama',
# )

# # --- –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –∏ —Ä–µ–∂–µ–º –Ω–∞ –∫—É—Å–æ—á–∫–∏ ---
# def load_and_chunk_file(filepath='C:\Users\ASUS\OneDrive\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\–£—á–±–∞\–ö—Ä–∞—Å–Ω–∞—è —Ç–∞–±–ª–µ—Ç–∫–∞\DataScience\–ü–†–û–ï–ö–¢–´\LLM_Lama\data.txt' , chunk_size=200):
    
#     print(f"üìÇ –ß–∏—Ç–∞—é —Ñ–∞–π–ª {filepath}...")
#     try:
#         with open(filepath, 'r', encoding='utf-8') as f:
#             text = f.read()
#     except FileNotFoundError:
#         print("‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª data.txt –Ω–µ –Ω–∞–π–¥–µ–Ω! –°–æ–∑–¥–∞–π –µ–≥–æ.")
#         return []

#     # –ü—Ä–æ—Å—Ç–∞—è –Ω–∞—Ä–µ–∑–∫–∞: –¥–µ–ª–∏–º —Ç–µ–∫—Å—Ç –∫–∞–∂–¥—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
#     # –í –ø—Ä–æ—Ñ–∏-—Å–∏—Å—Ç–µ–º–∞—Ö —Ä–µ–∂—É—Ç —É–º–Ω–µ–µ (–ø–æ —Ç–æ—á–∫–∞–º, –∞–±–∑–∞—Ü–∞–º), –Ω–æ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ö–≤–∞—Ç–∏—Ç –∏ —Ç–∞–∫.
#     chunks = []
#     for i in range(0, len(text), chunk_size):
#         chunk = text[i : i + chunk_size]
#         chunks.append(chunk)
    
#     print(f"üî™ –¢–µ–∫—Å—Ç –Ω–∞—Ä–µ–∑–∞–Ω –Ω–∞ {len(chunks)} –∫—É—Å–æ—á–∫–æ–≤ (—á–∞–Ω–∫–æ–≤).")
#     return chunks

# # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞
# documents = load_and_chunk_file()

# if not documents:
#     exit() # –ï—Å–ª–∏ —Ñ–∞–π–ª –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω - –≤—ã—Ö–æ–¥–∏–º

# print("üìö –°–æ–∑–¥–∞—é –∏–Ω–¥–µ–∫—Å—ã...")
# # –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
# def get_embedding(text):
#     return client.embeddings.create(model="all-minilm", input=text).data[0].embedding

# # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º —á–∞–Ω–∫–∏
# doc_vectors = [get_embedding(doc) for doc in documents]
# print("‚úÖ –ì–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ!")

# # --- –¶–ò–ö–õ ---
# while True:
#     user_query = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å –ø–æ —Ñ–∞–π–ª—É (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ")
#     if user_query.lower() in ["–≤—ã—Ö–æ–¥", "exit"]: break

#     # –ü–æ–∏—Å–∫
#     query_vector = get_embedding(user_query)
#     similarities = cosine_similarity([query_vector], doc_vectors)[0]
    
#     # –ë–µ—Ä–µ–º –¢–û–ü-1 –ª—É—á—à–∏–π –∫—É—Å–æ—á–µ–∫
#     best_idx = np.argmax(similarities)
#     best_doc = documents[best_idx]
#     score = similarities[best_idx]

#     print(f"   (–ù–∞–π–¥–µ–Ω —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é {score:.2f})")

#     # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
#     prompt = f"–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞: '{best_doc}'. –í–æ–ø—Ä–æ—Å: {user_query}"
    
#     stream = client.chat.completions.create(
#         model="mistral",
#         messages=[{"role": "user", "content": prompt}],
#         stream=True
#     )
    
#     print("ü§ñ –ë–æ—Ç: ", end="")
#     for chunk in stream:
#         if chunk.choices[0].delta.content:
#             print(chunk.choices[0].delta.content, end="", flush=True)
#     print("")